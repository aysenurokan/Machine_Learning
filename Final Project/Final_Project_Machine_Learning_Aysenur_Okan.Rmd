---
title: "Prediction of Suicidal Ideation"
author: "Aysenur Okan"
date: "2024-04-16"
output: html_document
---
  
``` {r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rpart)
library(rpart.plot)
library(partykit)
library(caret)
library(e1071)
library(party)
library(randomForest)
library(readr)
library(tidyverse)

#"sgroup", "sex", "marital", "gender", "ethnicity", "education", "race",


setwd("/Users/aysenur/Documents/GitHub/Machine_Learning/Final Project")
df <- read_csv("/Users/aysenur/Documents/GitHub/Machine_Learning/Final Project/ml_df.csv")
df <- df %>% dplyr::select(-PDWEMA, -SIEMA, - c("interperinteract_yesno", "SI", "surveytype", "EMA_SIdod", "sgroup"))
df <- na.omit(df)
df <- df[2:26]

df$SI_Binary <- factor(df$SI_Binary) #pools together passive and active SI

columns_to_factor <- c("sex", "race", "ethnicity", "marital", "gender")
df[columns_to_factor] <- lapply(df[columns_to_factor], factor)

#df$PDWEMA <- factor(df$PDWEMA) #Passive SI
#df$SIEMA <- factor(df$SIEMA) #Active SI 

#check that all predictors are numeric
str(df)

#Split data into training and testing
set.seed(150) # Set a seed for reproducibility
index <- createDataPartition(df$SI_Binary, p = 0.5, list = FALSE) # Create an index for splitting the data; use 50% for training and 50% for testing

# Create training and testing datasets
df <- df[index, ] #training data
tsdf <- df[-index, ] #testing data


# Define tuning routine with caret
trctrl=trainControl(method='cv',number=5) #k-fold cross-validation to determine the tuning parameters.
```



# Decision Trees

## Fit decision tree using rpart

The biggest predictors of suicidal ideation were feeling sad during the interaction (>3) and Negative Affectivity (>2.3). Then, I got the complexity parameter to see if my model overfit the data and could improve by pruning. The complexity parameter was very small (cp = 0.011), indicating that the model did not suffer from complexity.

```{r}
set.seed(41)
tree.rpart=rpart(SI_Binary~. -id,data=df) #predict suicidal ideation choice from all predictors 
prp(tree.rpart) #plot the decision tree
plotcp(tree.rpart)
#(1 s.e. from minimum error in the left out fold) 
printcp(tree.rpart) #similar, but now with numbers, not the plot

```



## Prune tree

To exercise pruning, I use the complexity parameter that was determined via k-fold cross-validation above, an re-plot the tree. All the values are the same as above. 

```{r}
tree.rpartp=prune(tree.rpart,cp=.013) #extract the tree with the determined complexity parameter; #adding "p" to signal that it is the pruned one
prp(tree.rpartp) #plot the pruned tree
```


## Examine the prediction error of the full tree with the test dataset

Next, I examine the prediction error of the full tree and the pruned tree in the testing dataset. 

```{r}
#prediction error for full tree 
pred.rpart=predict(tree.rpart,newdata=tsdf,type='class') #predicted values 
table(pred.rpart,tsdf$SI_Binary) #cross-tabulation table (observed values are in columns)
miss.rpart=1-mean(pred.rpart==tsdf$SI_Binary, na.rm = T) #0-1 loss 
miss.rpart # misclassification rate
```

## Examine the prediction error of the pruned tree in the testing dataset

These are the same because my full tree barely gave me a complexity parameter. Even though I used it to examine whether there would be a difference in prediction error, it was likely since the pruned tree is barely adjusted at all. 

```{r}
#prediction error for pruned tree
pred.rpartp=predict(tree.rpartp,newdata=tsdf,type='class') 
table(pred.rpartp,tsdf$SI_Binary)
miss.rpartp=1-mean(pred.rpartp==tsdf$SI_Binary, na.rm = T)
miss.rpartp
```


# Ensemble Methods


## Bagging with `randomForest()`

```{r}
bag1=randomForest(SI_Binary~.-id,data=df,mtry=23,ntree=800) #bagging model; `mtry` function argument controls the number of predictors considered every time the tree makes a split. If the `mtry` values equals the number of predictors, then the randomForest function does bagging.

bagpred=predict(bag1,tsdf,type='response') #obtain predicted values from testing dataset 
bag.table=table(bagpred,tsdf$SI_Binary) #cross-tabulation (rows are predicted, columns are observed 
bag.table

1-(mean(bagpred==tsdf$SI_Binary))
varImpPlot(bag1) #plot variable importance measures
```



## Random Forest

```{r}
# Predicting SI_Binary with random forest on the training dataset
rf=randomForest(SI_Binary~.-id,data=df,mtry=7,ntree=800) #use 7 vars at a time (7 random predictors as candidates to split on), 400 trees grown in bootstrapped datasets. 
#obtain the predicted values in the testing dataset
rfpred=predict(rf,tsdf,type='response')
# cross-tabulation of the classes
rf.table=table(rfpred,tsdf$SI_Binary)
#the variable importance measures.
rf.table
1-(mean(rfpred==tsdf$SI_Binary))
varImpPlot(rf)
```




### try 5 tuning parameters and the previously-specified cross-validation routine.
```{r}
set.seed(2000)
#Fit Model 
train.rf=train(SI_Binary~.,data=df,method='rf',tuneLength=5,trControl=trctrl) 
#See tuning parameters
train.rf
plot(train.rf)

final.rf=train.rf$finalModel
varImpPlot(final.rf)
```



### Fit the random forest with mtry = 9

```{r}
# Predicting SI_Binary with random forest on the training dataset
rf=randomForest(SI_Binary~.-id,data=df,mtry=9,ntree=800) #use 7 vars at a time (7 random predictors as candidates to split on), 400 trees grown in bootstrapped datasets. 
#obtain the predicted values in the testing dataset
rfpred=predict(rf,tsdf,type='response')
# cross-tabulation of the classes
rf.table=table(rfpred,tsdf$SI_Binary)
#the variable importance measures.
rf.table
1-(mean(rfpred==tsdf$SI_Binary))
varImpPlot(rf)
```


### Fit the random forest with mtry = 9 and add ID as a cluster variable

```{r}
# Predicting SI_Binary with random forest on the training dataset
rf=randomForest(SI_Binary~.,data=df,mtry=9,ntree=800) #use 7 vars at a time (7 random predictors as candidates to split on), 400 trees grown in bootstrapped datasets. 
#obtain the predicted values in the testing dataset
rfpred=predict(rf,tsdf,type='response')
# cross-tabulation of the classes
rf.table=table(rfpred,tsdf$SI_Binary)
#the variable importance measures.
rf.table
1-(mean(rfpred==tsdf$SI_Binary))
varImpPlot(rf)
```


# Conditional Inference Random Forest with cforest()

```{r}
df[] <- lapply(df, function(x) if(is.character(x)) factor(x) else x)
tsdf[] <- lapply(tsdf, function(x) if(is.character(x)) factor(x) else x)

#crf1 <- cforest(SI_Binary ~ . - id, data = df, controls = cforest_unbiased(ntree = 400, mtry = 3))
# Make predictions on the testing dataset

#crfpred <- predict(crf1, newdata = tsdf, type = "response") # Error in checkData(oldData, RET) :  Levels in factors of new data do not match original data -- I checked and the levels of the race variable are different. This is because there is only one American Indian participant in the entire dataset and they were added to the training data. 

#filter out the American Indian participant
df <- df %>% filter(race != "American Indian")
df$race <- factor(as.character(df$race, levels = "African American", "Asian","Pacific Islander" ,"White"))
tsdf$race <- factor(as.character(tsdf$race, levels = "African American", "Asian","Pacific Islander" ,"White"))
df[] <- lapply(df, function(x) if(is.character(x)) factor(x) else x)
crf1 <- cforest(SI_Binary ~ ., data = df, controls = cforest_unbiased(ntree = 800, mtry = 9))
crfpred <- predict(crf1, newdata = tsdf, type = "response")

crf.table=table(crfpred,tsdf$SI_Binary)
crf.table
1-(mean(crfpred==tsdf$SI_Binary))
vi=varimp(crf1)
dotchart(sort(vi,decreasing=FALSE))
```



## Tuning the Conditional Inference Random Forest


```{r}
#Fit model for conditional inference random forest
set.seed(2000)
train.crf=train(SI_Binary~.,data=df,method='cforest',tuneLength=5,trControl=trctrl)
train.crf
plot(train.crf)
final.crf=train.crf$finalModel

#plot variable importance
vi=varImp(final.crf)[,1]
names(vi)=rownames(varImp(final.crf))
dotchart(sort(vi,decreasing=FALSE))
```


### mtry = 24 conditional inference random forest

```{r}
crf1 <- cforest(SI_Binary ~ ., data = df, controls = cforest_unbiased(ntree = 800, mtry = 24))
crfpred <- predict(crf1, newdata = tsdf, type = "response")

crf.table=table(crfpred,tsdf$SI_Binary)
crf.table
1-(mean(crfpred==tsdf$SI_Binary))
vi=varimp(crf1)
dotchart(sort(vi,decreasing=FALSE))

```


